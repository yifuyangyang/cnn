# cnn_mnist_99plus.py
# ç¡®ä¿CNNåœ¨MNISTä¸Šæµ‹è¯•å‡†ç¡®ç‡â‰¥99%çš„æœ€ç»ˆç‰ˆä»£ç 
import time
import random
import numpy as np

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torchvision import datasets, transforms

import matplotlib.pyplot as plt

# =========================
# æ ¸å¿ƒé…ç½®ï¼ˆæ— éœ€ä¿®æ”¹ï¼Œå·²é”å®šæœ€ä¼˜å‚æ•°ï¼‰
# =========================
CONFIG = {
    "seed": 42,                # å›ºå®šç§å­ï¼Œç»“æœå¯å¤ç°
    "model": "cnn",            # é”å®šä¸ºCNN
    "batch_size": 64,          # æœ€ä¼˜æ‰¹å¤§å°
    "lr": 1e-3,                # å”¯ä¸€æœ€ä¼˜å­¦ä¹ ç‡
    "optimizer": "adam",       # Adamæ”¶æ•›æœ€å¿«
    "epochs": 10,              # CNNä»…éœ€10è½®å³å¯è¾¾æ ‡
    "save_plot": True,
    "plot_path": "results_cnn_99plus.png"
}

# =========================
# å·¥å…·å‡½æ•°ï¼ˆæ ¸å¿ƒåŠŸèƒ½ï¼Œæ— éœ€ä¿®æ”¹ï¼‰
# =========================
def set_seed(seed: int):
    """å›ºå®šæ‰€æœ‰éšæœºç§å­ï¼Œç¡®ä¿ç»“æœå¯å¤ç°"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

def count_params(model: nn.Module) -> int:
    """ç»Ÿè®¡æ¨¡å‹å¯è®­ç»ƒå‚æ•°é‡"""
    return sum(p.numel() for p in model.parameters() if p.requires_grad)

def build_optimizer(cfg, model):
    """åˆ›å»ºæœ€ä¼˜ä¼˜åŒ–å™¨ï¼ˆAdamï¼‰"""
    if cfg["optimizer"] == "adam":
        return torch.optim.Adam(model.parameters(), lr=cfg["lr"])
    else:
        raise ValueError("ä»…æ”¯æŒAdamä¼˜åŒ–å™¨ï¼Œå·²é”å®šæœ€ä¼˜é…ç½®")

def train_one_epoch(model, loader, optimizer, device):
    """è®­ç»ƒä¸€è½®ï¼Œè¿”å›å¹³å‡è®­ç»ƒæŸå¤±"""
    model.train()
    ce_loss = nn.CrossEntropyLoss()
    total_loss, total_samples = 0.0, 0

    for x, y in loader:
        x, y = x.to(device), y.to(device)
        
        optimizer.zero_grad()
        logits = model(x)
        loss = ce_loss(logits, y)
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item() * x.size(0)
        total_samples += x.size(0)
    
    return total_loss / total_samples

@torch.no_grad()
def evaluate(model, loader, device):
    """è¯„ä¼°æ¨¡å‹ï¼Œè¿”å›æµ‹è¯•æŸå¤±å’Œæµ‹è¯•å‡†ç¡®ç‡"""
    model.eval()
    ce_loss = nn.CrossEntropyLoss()
    total_loss, correct, total_samples = 0.0, 0, 0

    for x, y in loader:
        x, y = x.to(device), y.to(device)
        
        logits = model(x)
        loss = ce_loss(logits, y)
        
        total_loss += loss.item() * x.size(0)
        pred = logits.argmax(dim=1)
        correct += (pred == y).sum().item()
        total_samples += y.size(0)
    
    return total_loss / total_samples, correct / total_samples

# =========================
# æ ¸å¿ƒï¼šç¡®ä¿99%+çš„CNNæ¨¡å‹ï¼ˆå¸¦è½»é‡Dropouté˜²è¿‡æ‹Ÿåˆï¼‰
# =========================
class HighAccCNN(nn.Module):
    """
    ä¼˜åŒ–åçš„CNNç»“æ„ï¼š
    - å·ç§¯å±‚ï¼š32â†’64é€šé“ï¼ˆæå–è¶³å¤Ÿçš„å›¾åƒå±€éƒ¨ç‰¹å¾ï¼‰
    - æ± åŒ–å±‚ï¼š2x2 MaxPoolï¼ˆä¿ç•™å…³é”®ç‰¹å¾ï¼Œé™ä½ç»´åº¦ï¼‰
    - å…¨è¿æ¥å±‚ï¼š64*7*7â†’256â†’10ï¼ˆå……åˆ†åˆ©ç”¨å·ç§¯ç‰¹å¾ï¼‰
    - Dropoutï¼š0.25ï¼ˆè½»é‡æ­£åˆ™åŒ–ï¼Œé¿å…è¿‡æ‹Ÿåˆï¼‰
    """
    def __init__(self):
        super().__init__()
        # å·ç§¯å±‚ï¼ˆæ ¸å¿ƒï¼š32/64é€šé“ï¼‰
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        
        # æ¿€æ´»/æ± åŒ–/æ­£åˆ™åŒ–
        self.relu = nn.ReLU()
        self.pool = nn.MaxPool2d(2)
        self.dropout = nn.Dropout(0.25)  # è½»é‡Dropoutï¼Œç¡®ä¿å‡†ç¡®ç‡ç¨³å®š
        
        # å…¨è¿æ¥å±‚ï¼ˆæ ¸å¿ƒï¼š256ç¥ç»å…ƒï¼‰
        self.fc1 = nn.Linear(64 * 7 * 7, 256)
        self.fc2 = nn.Linear(256, 10)  # è¾“å‡º10ç±»ï¼ˆ0-9ï¼‰

    def forward(self, x):
        # x: [B, 1, 28, 28] â†’ å·ç§¯+æ± åŒ– â†’ [B, 32, 14, 14]
        x = self.pool(self.relu(self.conv1(x)))
        # â†’ å·ç§¯+æ± åŒ– â†’ [B, 64, 7, 7]
        x = self.pool(self.relu(self.conv2(x)))
        # Flatten â†’ [B, 64*7*7=3136]
        x = x.view(x.size(0), -1)
        # å…¨è¿æ¥+Dropout â†’ è¾“å‡º
        x = self.dropout(x)
        x = self.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)
        return x

# =========================
# ä¸»ç¨‹åºï¼ˆé€‚é…Windows CPUï¼Œæ— éœ€ä¿®æ”¹ï¼‰
# =========================
def main():
    # å›ºå®šç§å­ï¼Œç¡®ä¿ç»“æœå¯å¤ç°
    set_seed(CONFIG["seed"])
    # è®¾å¤‡é€‰æ‹©ï¼ˆCPUï¼Œé€‚é…ä½ çš„ç¯å¢ƒï¼‰
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print("="*60)
    print(f"è¿è¡Œç¯å¢ƒï¼š{device} | æ¨¡å‹ï¼šCNNï¼ˆç›®æ ‡â‰¥99%ï¼‰")
    print(f"è®­ç»ƒå‚æ•°ï¼šepochs={CONFIG['epochs']} | batch_size={CONFIG['batch_size']} | lr={CONFIG['lr']}")
    print("="*60)

    # åŠ è½½MNISTæ•°æ®é›†ï¼ˆä½¿ç”¨ä½ æ‰‹åŠ¨ä¸‹è½½çš„æœ¬åœ°æ•°æ®ï¼Œé¿å…é‡æ–°ä¸‹è½½ï¼‰
    transform = transforms.Compose([transforms.ToTensor()])
    # å…³é”®ï¼šdownload=Falseï¼ˆä½¿ç”¨æœ¬åœ°å·²ä¸‹è½½çš„æ•°æ®é›†ï¼‰
    train_ds = datasets.MNIST(
        root="./data", 
        train=True, 
        download=False,  # å¿…é¡»Falseï¼Œç”¨ä½ æ‰‹åŠ¨ä¸‹è½½çš„æ–‡ä»¶
        transform=transform
    )
    test_ds = datasets.MNIST(
        root="./data", 
        train=False, 
        download=False,  # å¿…é¡»False
        transform=transform
    )

    # æ•°æ®åŠ è½½å™¨ï¼ˆå…³é”®ï¼šnum_workers=0ï¼Œé€‚é…Windows CPUï¼‰
    train_loader = DataLoader(
        train_ds,
        batch_size=CONFIG["batch_size"],
        shuffle=True,
        num_workers=0,  # Windows CPUå¿…é¡»è®¾ä¸º0ï¼Œé¿å…æ•°æ®åŠ è½½å¼‚å¸¸
        pin_memory=False
    )
    test_loader = DataLoader(
        test_ds,
        batch_size=CONFIG["batch_size"],
        shuffle=False,
        num_workers=0,
        pin_memory=False
    )

    # åˆå§‹åŒ–æ¨¡å‹å’Œä¼˜åŒ–å™¨
    model = HighAccCNN().to(device)
    optimizer = build_optimizer(CONFIG, model)
    print(f"æ¨¡å‹å‚æ•°é‡ï¼š{count_params(model):,}ï¼ˆæœ€ä¼˜å¤æ‚åº¦ï¼‰")
    print("="*60)

    # è®°å½•è®­ç»ƒè¿‡ç¨‹
    train_losses, test_losses, test_accs = [], [], []
    start_time = time.time()

    # è®­ç»ƒ10è½®ï¼ˆè¶³å¤Ÿè¾¾æ ‡ï¼‰
    for epoch in range(1, CONFIG["epochs"] + 1):
        # è®­ç»ƒä¸€è½®
        train_loss = train_one_epoch(model, train_loader, optimizer, device)
        # è¯„ä¼°æµ‹è¯•é›†
        test_loss, test_acc = evaluate(model, test_loader, device)
        
        # è®°å½•æ•°æ®
        train_losses.append(train_loss)
        test_losses.append(test_loss)
        test_accs.append(test_acc)
        
        # æ‰“å°æ¯è½®ç»“æœï¼ˆæ¸…æ™°å±•ç¤ºå‡†ç¡®ç‡ï¼‰
        print(f"ç¬¬{epoch:2d}è½® | è®­ç»ƒæŸå¤±ï¼š{train_loss:.4f} | æµ‹è¯•æŸå¤±ï¼š{test_loss:.4f} | æµ‹è¯•å‡†ç¡®ç‡ï¼š{test_acc*100:.2f}%")

    # æœ€ç»ˆç»“æœç»Ÿè®¡
    total_time = time.time() - start_time
    final_acc = test_accs[-1] * 100
    print("="*60)
    print(f"âœ… æœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡ï¼š{final_acc:.2f}%ï¼ˆç›®æ ‡â‰¥99%ï¼‰")
    print(f"âœ… æ€»è®­ç»ƒæ—¶é—´ï¼š{total_time:.1f}ç§’ï¼ˆCPUç¯å¢ƒï¼‰")
    print("="*60)

    # ä¿å­˜è®­ç»ƒæ›²çº¿å›¾ï¼ˆè‹±æ–‡æ ‡ç­¾ï¼Œæ— å­—ä½“è­¦å‘Šï¼‰
    if CONFIG["save_plot"]:
        plt.figure(figsize=(10, 6))
        plt.plot(range(1, CONFIG["epochs"]+1), train_losses, label="Train Loss", marker="o", color="blue")
        plt.plot(range(1, CONFIG["epochs"]+1), test_losses, label="Test Loss", marker="s", color="orange")
        plt.plot(range(1, CONFIG["epochs"]+1), test_accs, label="Test Accuracy", marker="^", color="green")
        plt.xlabel("Epoch")
        plt.ylabel("Value")
        plt.legend(loc="lower right")
        plt.title(f"CNN MNIST | Final Accuracy: {final_acc:.2f}%")
        plt.grid(alpha=0.3)
        plt.savefig(CONFIG["plot_path"], dpi=160, bbox_inches="tight")
        print(f"ğŸ“Š è®­ç»ƒæ›²çº¿å›¾å·²ä¿å­˜è‡³ï¼š{CONFIG['plot_path']}")

if __name__ == "__main__":
    main()