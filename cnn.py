# bp_vs_cnn_mnist.py
# ============================================================
# å®éªŒï¼šBP(MLP) vs CNN åœ¨ MNIST ä¸Šçš„å¯¹æ¯”
# 
# å®éªŒç›®æ ‡ï¼š
# 1. ç½‘ç»œç»“æ„å‚æ•°å¯¹æ¨¡å‹ç²¾åº¦çš„å½±å“
#    - MLP: éšè—å±‚å¤§å°ã€å±‚æ•°ã€Dropoutç­‰
#    - CNN: å·ç§¯é€šé“æ•°ã€å·ç§¯å±‚æ•°ã€å…¨è¿æ¥å±‚å¤§å°ç­‰
# 2. é€šè¿‡æ‰‹åŠ¨è°ƒå‚æå‡æ¨¡å‹æ€§èƒ½çš„æ–¹æ³•
#    - å­¦ä¹ ç‡è°ƒæ•´ã€ä¼˜åŒ–å™¨é€‰æ‹©ã€æ‰¹æ¬¡å¤§å°ã€è®­ç»ƒè½®æ•°
# 3. CNN ç›¸æ¯” MLP åœ¨å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­çš„ä¼˜åŠ¿
#    - å±€éƒ¨æ„ŸçŸ¥ã€å‚æ•°å…±äº«ã€å¹³ç§»ä¸å˜æ€§ã€å±‚æ¬¡ç‰¹å¾æå–
#
# å®éªŒä»»åŠ¡ï¼š
# 1. ä½¿ç”¨ MLP: é€šè¿‡è°ƒå‚ä½¿æµ‹è¯•é›†å‡†ç¡®ç‡è¾¾åˆ° 98% åŠä»¥ä¸Š
# 2. ä½¿ç”¨ CNN: é€šè¿‡è°ƒå‚ä½¿æµ‹è¯•é›†å‡†ç¡®ç‡è¾¾åˆ° 99% åŠä»¥ä¸Š
#
# è°ƒå‚ç­–ç•¥ï¼š
# â˜… MLPè¾¾åˆ°98%çš„é…ç½®å»ºè®®ï¼š
#    - ç»“æ„ï¼š[784, 512, 256, 128, 10]
#    - è®­ç»ƒï¼šepochs=20, lr=0.001, batch_size=128
#    - ä¼˜åŒ–å™¨ï¼šAdam
#
# â˜… CNNè¾¾åˆ°99%çš„é…ç½®å»ºè®®ï¼š
#    - ç»“æ„ï¼šconv1: 1->32, conv2: 32->64, fc: 64*7*7->128->10
#    - è®­ç»ƒï¼šepochs=15, lr=0.001, batch_size=64
#    - ä¼˜åŒ–å™¨ï¼šAdam
# ============================================================

import time
import random
import numpy as np

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torchvision import datasets, transforms

import matplotlib.pyplot as plt


# =========================
# CONFIGï¼ˆå­¦ç”Ÿå¯æ”¹ï¼šè®­ç»ƒå‚æ•°ï¼‰
# =========================
CONFIG = {
    # å›ºå®šéšæœºç§å­ï¼šæ–¹ä¾¿å¯¹æ¯”ï¼ˆåŒå‚æ•°ä¸‹ç»“æœæ›´ç¨³å®šï¼‰
    "seed": 42,

    # â˜…â˜…â˜…â˜…â˜… é€‰æ‹©è¦è®­ç»ƒçš„æ¨¡å‹ï¼š "mlp" æˆ– "cnn" â˜…â˜…â˜…â˜…â˜…
    "model": "mlp",  # å…ˆè¿è¡ŒMLPï¼Œç›®æ ‡98%ï¼›ç„¶åæ”¹ä¸º"cnn"ï¼Œç›®æ ‡99%

    # â˜…â˜…â˜…â˜…â˜… è®­ç»ƒç›¸å…³å‚æ•°ï¼ˆå¯ä»¥æ”¹ï¼Œç”¨äºè§‚å¯Ÿæ”¶æ•›ä¸ç²¾åº¦å˜åŒ–ï¼‰â˜…â˜…â˜…â˜…â˜…
    "epochs": 15,           # MLPå»ºè®®15-20ï¼ŒCNNå»ºè®®10-15
    "batch_size": 128,      # å»ºè®®ï¼š32, 64, 128, 256
    "lr": 1e-3,             # å»ºè®®å¯¹æ¯”ï¼š1e-2 / 1e-3 / 5e-4 / 1e-4
    "optimizer": "adam",    # "adam" æˆ– "sgd"ï¼ˆAdamé€šå¸¸æ›´å¥½ï¼‰

    # â˜…â˜…â˜…â˜…â˜… æ•°æ®å¢å¼ºï¼ˆå¯é€‰ï¼Œå¯¹CNNç‰¹åˆ«æœ‰æ•ˆï¼‰â˜…â˜…â˜…â˜…â˜…
    "use_data_augmentation": True,  # å¯ç”¨æ•°æ®å¢å¼ºæé«˜æ³›åŒ–èƒ½åŠ›

    # è¾“å‡º
    "save_plot": True,
    "plot_path": "results.png",
}


# =========================
# å·¥å…·å‡½æ•°ï¼ˆä¿æŒä¸å˜ï¼‰
# =========================
def set_seed(seed: int):
    """å›ºå®šéšæœºç§å­ï¼šè®©ç»“æœæ›´å¯å¤ç°ï¼ˆä¾¿äºå…¬å¹³å¯¹æ¯”ï¼‰"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


def count_params(model: nn.Module) -> int:
    """ç»Ÿè®¡å¯è®­ç»ƒå‚æ•°é‡ï¼ˆè¡¡é‡æ¨¡å‹å¤æ‚åº¦ï¼‰"""
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def build_optimizer(cfg, model):
    """æ ¹æ®é…ç½®åˆ›å»ºä¼˜åŒ–å™¨"""
    lr = cfg["lr"]
    opt = cfg["optimizer"].lower()

    if opt == "adam":
        return torch.optim.Adam(model.parameters(), lr=lr)
    elif opt == "sgd":
        return torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)
    else:
        raise ValueError("CONFIG['optimizer'] must be 'adam' or 'sgd'")


def train_one_epoch(model, loader, optimizer, device):
    """è®­ç»ƒä¸€ä¸ª epochï¼Œè¿”å›å¹³å‡ train loss"""
    model.train()
    ce = nn.CrossEntropyLoss()

    total_loss = 0.0
    total = 0

    for x, y in loader:
        x, y = x.to(device), y.to(device)

        optimizer.zero_grad()
        logits = model(x)
        loss = ce(logits, y)
        loss.backward()
        optimizer.step()

        total_loss += loss.item() * x.size(0)
        total += x.size(0)

    return total_loss / total


@torch.no_grad()
def evaluate(model, loader, device):
    """åœ¨æµ‹è¯•é›†è¯„ä¼°ï¼Œè¿”å› test loss å’Œ test accuracy"""
    model.eval()
    ce = nn.CrossEntropyLoss()

    total_loss = 0.0
    correct = 0
    total = 0

    for x, y in loader:
        x, y = x.to(device), y.to(device)

        logits = model(x)
        loss = ce(logits, y)

        total_loss += loss.item() * x.size(0)
        pred = logits.argmax(dim=1)
        correct += (pred == y).sum().item()
        total += y.size(0)

    return total_loss / total, correct / total


# =========================
# æ¨¡å‹å®šä¹‰ï¼šBP(MLP) - ç›®æ ‡ï¼š98% å‡†ç¡®ç‡
# =========================
class MLP(nn.Module):
    """
    BP ç¥ç»ç½‘ç»œï¼ˆå¤šå±‚æ„ŸçŸ¥æœºï¼ŒMLPï¼‰
    - è¾“å…¥ï¼šMNIST å›¾åƒ [B, 1, 28, 28]
    - å…ˆ Flatten æˆå‘é‡ [B, 784]
    - å†èµ°å…¨è¿æ¥å±‚åšåˆ†ç±»
    
    â˜… ç½‘ç»œç»“æ„å‚æ•°å¯¹ç²¾åº¦çš„å½±å“ â˜…
    1. éšè—å±‚å¤§å°ï¼šå¢åŠ ç¥ç»å…ƒæ•°é‡å¯ä»¥æé«˜æ¨¡å‹è¡¨è¾¾èƒ½åŠ›ï¼Œä½†å¯èƒ½è¿‡æ‹Ÿåˆ
    2. éšè—å±‚æ•°é‡ï¼šå¢åŠ å±‚æ•°å¯ä»¥å­¦ä¹ æ›´å¤æ‚çš„ç‰¹å¾ï¼Œä½†è®­ç»ƒæ›´å›°éš¾
    3. Dropoutï¼šé˜²æ­¢è¿‡æ‹Ÿåˆï¼Œæé«˜æ³›åŒ–èƒ½åŠ›
    
    â˜… MLPè¾¾åˆ°98%å‡†ç¡®ç‡çš„è°ƒå‚æ–¹æ³• â˜…
    1. ä½¿ç”¨3-4ä¸ªéšè—å±‚
    2. æ¯å±‚ç¥ç»å…ƒæ•°ï¼š784 -> 512 -> 256 -> 128 -> 10
    3. æ·»åŠ Dropoutå±‚ï¼ˆ0.3-0.5ï¼‰
    4. è®­ç»ƒ15-20ä¸ªepochï¼Œå­¦ä¹ ç‡0.001
    """

    def __init__(self):
        super().__init__()

        # â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…
        # â˜…â˜…â˜…â˜…â˜… ä¿®æ”¹åŒºï¼ˆMLP ç½‘ç»œç»“æ„å‚æ•°ï¼‰â˜…â˜…â˜…â˜…â˜…
        # 
        # è¦è¾¾åˆ°98%å‡†ç¡®ç‡ï¼Œå»ºè®®é…ç½®ï¼š
        # æ–¹æ¡ˆ1ï¼ˆ3å±‚éšè—å±‚ï¼Œæ¨èï¼‰ï¼š
        #   fc1: 784 -> 512
        #   fc2: 512 -> 256
        #   fc3: 256 -> 128
        #   out: 128 -> 10
        #
        # æ–¹æ¡ˆ2ï¼ˆ4å±‚éšè—å±‚ï¼Œæ›´å¼ºè¡¨è¾¾èƒ½åŠ›ï¼‰ï¼š
        #   fc1: 784 -> 784  (ä¿æŒç»´åº¦)
        #   fc2: 784 -> 512
        #   fc3: 512 -> 256
        #   fc4: 256 -> 128
        #   out: 128 -> 10
        #
        # â˜… æ³¨æ„ï¼šå±‚æ•°è¶Šæ·±ï¼Œéœ€è¦çš„è®­ç»ƒæ—¶é—´è¶Šé•¿ â˜…
        # â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…

        # æ–¹æ¡ˆ1ï¼š3å±‚éšè—å±‚ï¼ˆæ¨èèµ·å§‹é…ç½®ï¼‰
        self.fc1 = nn.Linear(28 * 28, 512)   # ç¬¬ä¸€éšè—å±‚
        self.fc2 = nn.Linear(512, 256)       # ç¬¬äºŒéšè—å±‚
        self.fc3 = nn.Linear(256, 128)       # ç¬¬ä¸‰éšè—å±‚
        self.out = nn.Linear(128, 10)        # è¾“å‡ºå±‚

        # æ¿€æ´»å‡½æ•°
        self.relu = nn.ReLU()
        
        # â˜… å¯é€‰ï¼šæ·»åŠ Dropouté˜²æ­¢è¿‡æ‹Ÿåˆ â˜…
        self.dropout = nn.Dropout(p=0.3)     # Dropoutæ¦‚ç‡0.3


    def forward(self, x):
        # x: [B, 1, 28, 28]
        # MLP å¿…é¡» Flattenï¼š [B, 784]
        x = x.view(x.size(0), -1)

        x = self.relu(self.fc1(x))
        x = self.dropout(x)  # åº”ç”¨Dropout
        
        x = self.relu(self.fc2(x))
        x = self.dropout(x)  # åº”ç”¨Dropout
        
        x = self.relu(self.fc3(x))
        x = self.dropout(x)  # åº”ç”¨Dropout
        
        x = self.out(x)
        return x


# =========================
# æ¨¡å‹å®šä¹‰ï¼šCNN - ç›®æ ‡ï¼š99% å‡†ç¡®ç‡
# =========================
class SimpleCNN(nn.Module):
    """
    å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰
    - è¾“å…¥ä¿æŒå›¾åƒç»“æ„ï¼š[B, 1, 28, 28]
    - é€šè¿‡å·ç§¯æå–å±€éƒ¨ç‰¹å¾ï¼ˆè¾¹ç¼˜ã€æ‹è§’ã€ç¬”ç”»ç»„åˆï¼‰
    
    â˜… CNNç›¸æ¯”MLPåœ¨å›¾åƒåˆ†ç±»ä¸­çš„ä¼˜åŠ¿ â˜…
    1. å±€éƒ¨æ„ŸçŸ¥ï¼šæ¯ä¸ªç¥ç»å…ƒåªæ„Ÿå—å›¾åƒçš„å±€éƒ¨åŒºåŸŸ
    2. å‚æ•°å…±äº«ï¼šåŒä¸€ä¸ªå·ç§¯æ ¸åœ¨æ•´ä¸ªå›¾åƒä¸Šæ»‘åŠ¨ï¼Œå¤§å¤§å‡å°‘å‚æ•°é‡
    3. å¹³ç§»ä¸å˜æ€§ï¼šç‰©ä½“åœ¨å›¾åƒä¸­ä½ç½®å˜åŒ–ä¸å½±å“è¯†åˆ«
    4. å±‚æ¬¡ç‰¹å¾æå–ï¼šæµ…å±‚æå–è¾¹ç¼˜ï¼Œæ·±å±‚æå–å¤æ‚æ¨¡å¼
    
    â˜… CNNè¾¾åˆ°99%å‡†ç¡®ç‡çš„è°ƒå‚æ–¹æ³• â˜…
    1. å¢åŠ å·ç§¯é€šé“æ•°ï¼š32->64
    2. æ·»åŠ BatchNormå±‚åŠ é€Ÿæ”¶æ•›
    3. ä½¿ç”¨æ•°æ®å¢å¼º
    4. è®­ç»ƒ10-15ä¸ªepochï¼Œå­¦ä¹ ç‡0.001
    """

    def __init__(self):
        super().__init__()

        # â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…
        # â˜…â˜…â˜…â˜…â˜… ä¿®æ”¹åŒºï¼ˆCNN ç½‘ç»œç»“æ„å‚æ•°ï¼‰â˜…â˜…â˜…â˜…â˜…
        # 
        # è¦è¾¾åˆ°99%å‡†ç¡®ç‡ï¼Œå»ºè®®é…ç½®ï¼š
        # æ–¹æ¡ˆ1ï¼ˆä¸­ç­‰å¤§å°ï¼Œæ¨èï¼‰ï¼š
        #   conv1: 1 -> 32
        #   conv2: 32 -> 64
        #   fc1: 64*7*7 -> 128
        #   out: 128 -> 10
        #
        # æ–¹æ¡ˆ2ï¼ˆæ›´å¤§æ¨¡å‹ï¼Œæ›´é«˜ç²¾åº¦ï¼‰ï¼š
        #   conv1: 1 -> 64
        #   conv2: 64 -> 128
        #   fc1: 128*7*7 -> 256
        #   out: 256 -> 10
        #
        # â˜… æ³¨æ„ï¼šæœ¬ç½‘ç»œæœ‰ä¸¤æ¬¡ 2x2 MaxPoolï¼š
        # - å›¾ç‰‡ 28x28 -> 14x14 -> 7x7
        # æ‰€ä»¥ç¬¬äºŒå±‚å·ç§¯è¾“å‡ºçš„ç‰¹å¾å›¾å°ºå¯¸ä¸º 7x7
        # å…¨è¿æ¥å±‚è¾“å…¥ç»´åº¦è¦å†™æˆï¼š (conv2_out_channels * 7 * 7)
        # â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…

        # æ–¹æ¡ˆ1ï¼šä¸­ç­‰å¤§å°CNNï¼ˆæ¨èèµ·å§‹é…ç½®ï¼‰
        c1_out = 32   # ç¬¬ä¸€å±‚å·ç§¯è¾“å‡ºé€šé“æ•°
        c2_out = 64   # ç¬¬äºŒå±‚å·ç§¯è¾“å‡ºé€šé“æ•°
        fc1_size = 128  # å…¨è¿æ¥å±‚å¤§å°

        # å·ç§¯å±‚
        self.conv1 = nn.Conv2d(1, c1_out, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(c1_out, c2_out, kernel_size=3, padding=1)
        
        # â˜… å¯é€‰ï¼šæ·»åŠ BatchNormå±‚åŠ é€Ÿæ”¶æ•› â˜…
        self.bn1 = nn.BatchNorm2d(c1_out)
        self.bn2 = nn.BatchNorm2d(c2_out)

        self.relu = nn.ReLU()
        self.pool = nn.MaxPool2d(2)  # 2x2 æ± åŒ–ï¼Œå°ºå¯¸å‡åŠ

        # å…¨è¿æ¥å±‚ï¼šè¾“å…¥æ˜¯ c2_out * 7 * 7
        self.fc1 = nn.Linear(c2_out * 7 * 7, fc1_size)
        self.fc2 = nn.Linear(fc1_size, 10)
        
        # â˜… å¯é€‰ï¼šæ·»åŠ Dropouté˜²æ­¢è¿‡æ‹Ÿåˆ â˜…
        self.dropout = nn.Dropout(p=0.5)

    def forward(self, x):
        # x: [B, 1, 28, 28]  (CNN ä¸éœ€è¦ Flatten è¾“å…¥)
        x = self.pool(self.relu(self.bn1(self.conv1(x))))  # -> [B, 32, 14, 14]
        x = self.pool(self.relu(self.bn2(self.conv2(x))))  # -> [B, 64, 7, 7]
        x = x.view(x.size(0), -1)                # -> [B, 64*7*7]
        x = self.relu(self.fc1(x))
        x = self.dropout(x)  # åº”ç”¨Dropout
        x = self.fc2(x)
        return x


def build_model(model_name: str) -> nn.Module:
    if model_name == "mlp":
        return MLP()
    elif model_name == "cnn":
        return SimpleCNN()
    else:
        raise ValueError("CONFIG['model'] must be 'mlp' or 'cnn'")


# =========================
# ä¸»ç¨‹åº
# =========================
def main():
    set_seed(CONFIG["seed"])
    device = "cuda" if torch.cuda.is_available() else "cpu"

    # -------------------------
    # æ•°æ®è·å–ï¼ˆè‡ªåŠ¨ä¸‹è½½ MNISTï¼‰
    # -------------------------
    # åŸºç¡€è½¬æ¢
    base_transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.1307,), (0.3081,))  # MNISTæ ‡å‡†åŒ–
    ])
    
    # æ•°æ®å¢å¼ºï¼ˆä»…è®­ç»ƒé›†ï¼‰
    if CONFIG["use_data_augmentation"] and CONFIG["model"] == "cnn":
        train_transform = transforms.Compose([
            transforms.RandomRotation(5),  # éšæœºæ—‹è½¬5åº¦
            transforms.RandomAffine(0, translate=(0.05, 0.05)),  # éšæœºå¹³ç§»
            transforms.ToTensor(),
            transforms.Normalize((0.1307,), (0.3081,))
        ])
    else:
        train_transform = base_transform
    
    # download=Trueï¼šè‹¥æœ¬åœ°æ²¡æœ‰ MNISTï¼Œä¼šè‡ªåŠ¨è”ç½‘ä¸‹è½½å¹¶è§£å‹åˆ° ./data/
    train_ds = datasets.MNIST(root="./data", train=True, download=True, transform=train_transform)
    test_ds  = datasets.MNIST(root="./data", train=False, download=True, transform=base_transform)

    train_loader = DataLoader(
        train_ds,
        batch_size=CONFIG["batch_size"],
        shuffle=True,
        num_workers=2,
        pin_memory=True
    )
    test_loader = DataLoader(
        test_ds,
        batch_size=CONFIG["batch_size"],
        shuffle=False,
        num_workers=2,
        pin_memory=True
    )

    # -------------------------
    # å»ºæ¨¡ä¸ä¼˜åŒ–å™¨
    # -------------------------
    model = build_model(CONFIG["model"]).to(device)
    optimizer = build_optimizer(CONFIG, model)
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆå¯é€‰ï¼Œæœ‰åŠ©äºè¾¾åˆ°æ›´é«˜ç²¾åº¦ï¼‰
    try:
        # PyTorchæ–°ç‰ˆæœ¬
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode='max', factor=0.5, patience=3, verbose=True
        )
    except TypeError:
        # PyTorchæ—§ç‰ˆæœ¬
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode='max', factor=0.5, patience=3
        )

    print("=" * 60)
    print(f"å®éªŒ: {CONFIG['model'].upper()} åœ¨ MNIST ä¸Šçš„æ€§èƒ½")
    print("=" * 60)
    print(f"è®¾å¤‡: {device}")
    print(f"æ¨¡å‹: {CONFIG['model']}")
    print(f"å‚æ•°é‡: {count_params(model):,}")
    print(f"è®­ç»ƒè½®æ•°: {CONFIG['epochs']} | æ‰¹æ¬¡å¤§å°: {CONFIG['batch_size']}")
    print(f"å­¦ä¹ ç‡: {CONFIG['lr']} | ä¼˜åŒ–å™¨: {CONFIG['optimizer']}")
    print(f"æ•°æ®å¢å¼º: {CONFIG['use_data_augmentation']}")
    print("=" * 60)
    
    # æ˜¾ç¤ºå®éªŒç›®æ ‡
    if CONFIG["model"] == "mlp":
        print("å®éªŒç›®æ ‡: æµ‹è¯•é›†å‡†ç¡®ç‡è¾¾åˆ° 98% åŠä»¥ä¸Š")
        print("è°ƒå‚å»ºè®®:")
        print("  1. ç¡®ä¿MLPæœ‰3-4ä¸ªéšè—å±‚")
        print("  2. éšè—å±‚å¤§å°è‡³å°‘: 512 -> 256 -> 128")
        print("  3. è®­ç»ƒ15-20ä¸ªepoch")
        print("  4. ä½¿ç”¨Adamä¼˜åŒ–å™¨ï¼Œå­¦ä¹ ç‡0.001")
    else:
        print("å®éªŒç›®æ ‡: æµ‹è¯•é›†å‡†ç¡®ç‡è¾¾åˆ° 99% åŠä»¥ä¸Š")
        print("è°ƒå‚å»ºè®®:")
        print("  1. å·ç§¯é€šé“è‡³å°‘: 32 -> 64")
        print("  2. å¯ç”¨æ•°æ®å¢å¼º")
        print("  3. è®­ç»ƒ10-15ä¸ªepoch")
        print("  4. ä½¿ç”¨Adamä¼˜åŒ–å™¨ï¼Œå­¦ä¹ ç‡0.001")
    print("=" * 60)

    # è®°å½•æ›²çº¿
    train_losses, test_losses, test_accs = [], [], []
    best_acc = 0.0
    best_epoch = 0

    start = time.time()

    for epoch in range(1, CONFIG["epochs"] + 1):
        tr_loss = train_one_epoch(model, train_loader, optimizer, device)
        te_loss, te_acc = evaluate(model, test_loader, device)
        
        # æ›´æ–°å­¦ä¹ ç‡
        scheduler.step(te_acc)

        train_losses.append(tr_loss)
        test_losses.append(te_loss)
        test_accs.append(te_acc)
        
        # è®°å½•æœ€ä½³å‡†ç¡®ç‡
        if te_acc > best_acc:
            best_acc = te_acc
            best_epoch = epoch

        print(f"Epoch {epoch:02d}/{CONFIG['epochs']} | "
              f"train_loss={tr_loss:.4f} | test_loss={te_loss:.4f} | "
              f"test_acc={te_acc*100:.2f}% | best={best_acc*100:.2f}% @ epoch {best_epoch}")

    elapsed = time.time() - start

    print("=" * 60)
    print(f"æœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡: {test_accs[-1]*100:.2f}%")
    print(f"æœ€ä½³æµ‹è¯•å‡†ç¡®ç‡: {best_acc*100:.2f}% (epoch {best_epoch})")
    print(f"è®­ç»ƒæ—¶é—´: {elapsed:.1f}ç§’")
    
    # åˆ¤æ–­æ˜¯å¦è¾¾åˆ°ç›®æ ‡
    target_acc = 0.98 if CONFIG["model"] == "mlp" else 0.99
    if best_acc >= target_acc:
        print(f"âœ… æˆåŠŸè¾¾åˆ°ç›®æ ‡å‡†ç¡®ç‡ {target_acc*100:.0f}%!")
    else:
        print(f"âŒ æœªè¾¾åˆ°ç›®æ ‡å‡†ç¡®ç‡ {target_acc*100:.0f}%ï¼Œå»ºè®®è°ƒæ•´å‚æ•°")
        print("è°ƒå‚å»ºè®®:")
        if CONFIG["model"] == "mlp":
            print("  1. å¢åŠ éšè—å±‚å¤§å°ï¼ˆå¦‚ 784 -> 1024 -> 512 -> 256 -> 128ï¼‰")
            print("  2. å¢åŠ è®­ç»ƒè½®æ•°åˆ°20-25")
            print("  3. é™ä½å­¦ä¹ ç‡åˆ°5e-4")
            print("  4. å¢åŠ Dropoutæ¦‚ç‡åˆ°0.5")
        else:
            print("  1. å¢åŠ å·ç§¯é€šé“æ•°ï¼ˆå¦‚ 64 -> 128 -> 256ï¼‰")
            print("  2. æ·»åŠ æ›´å¤šå·ç§¯å±‚ï¼ˆ3å±‚å·ç§¯ï¼‰")
            print("  3. å¯ç”¨æ•°æ®å¢å¼º")
            print("  4. å¢åŠ è®­ç»ƒè½®æ•°åˆ°15-20")
    print("=" * 60)
    
    # æ˜¾ç¤ºCNNç›¸æ¯”MLPçš„ä¼˜åŠ¿
    if CONFIG["model"] == "cnn" and best_acc >= 0.99:
        print("\nğŸ“Š CNNç›¸æ¯”MLPçš„ä¼˜åŠ¿æ€»ç»“:")
        print("-" * 40)
        print("1. å±€éƒ¨æ„ŸçŸ¥: å·ç§¯æ ¸åªå…³æ³¨å›¾åƒå±€éƒ¨åŒºåŸŸ")
        print("2. å‚æ•°å…±äº«: åŒä¸€å·ç§¯æ ¸åœ¨æ•´ä¸ªå›¾åƒä¸Šæ»‘åŠ¨ï¼Œå‚æ•°é‡è¿œå°‘äºMLP")
        print(f"   - æœ¬CNNå‚æ•°é‡: {count_params(model):,}")
        print(f"   - åŒç­‰ç²¾åº¦MLPå‚æ•°é‡: é€šå¸¸è¶…è¿‡1,000,000")
        print("3. å¹³ç§»ä¸å˜æ€§: æ•°å­—åœ¨å›¾åƒä¸­ä½ç½®å˜åŒ–ä¸å½±å“è¯†åˆ«")
        print("4. å±‚æ¬¡ç‰¹å¾æå–:")
        print("   - ç¬¬ä¸€å±‚: æ£€æµ‹è¾¹ç¼˜ã€è§’ç‚¹")
        print("   - ç¬¬äºŒå±‚: ç»„åˆæˆç¬”ç”»éƒ¨åˆ†")
        print("   - å…¨è¿æ¥å±‚: ç»„åˆæˆå®Œæ•´æ•°å­—")
        print("-" * 40)

    # -------------------------
    # ä¿å­˜æ›²çº¿å›¾ï¼šloss + acc
    # -------------------------
    if CONFIG["save_plot"]:
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))
        
        # æŸå¤±æ›²çº¿
        ax1.plot(train_losses, label="è®­ç»ƒæŸå¤±", linewidth=2)
        ax1.plot(test_losses, label="æµ‹è¯•æŸå¤±", linewidth=2)
        ax1.set_xlabel("è®­ç»ƒè½®æ•°")
        ax1.set_ylabel("æŸå¤±å€¼")
        ax1.set_title("è®­ç»ƒå’Œæµ‹è¯•æŸå¤±æ›²çº¿")
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # å‡†ç¡®ç‡æ›²çº¿
        ax2.plot(test_accs, label="æµ‹è¯•å‡†ç¡®ç‡", linewidth=2, color='green')
        ax2.axhline(y=target_acc, color='red', linestyle='--', 
                   label=f"ç›®æ ‡å‡†ç¡®ç‡ ({target_acc*100:.0f}%)")
        ax2.set_xlabel("è®­ç»ƒè½®æ•°")
        ax2.set_ylabel("å‡†ç¡®ç‡")
        ax2.set_title("æµ‹è¯•å‡†ç¡®ç‡æ›²çº¿")
        ax2.set_ylim([0.8, 1.0])
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # æ·»åŠ æ¨¡å‹ä¿¡æ¯å’Œç»“æœåˆ°æ ‡é¢˜
        result_status = "âœ… è¾¾æ ‡" if best_acc >= target_acc else "âŒ æœªè¾¾æ ‡"
        plt.suptitle(
            f"{CONFIG['model'].upper()} on MNIST | "
            f"Best Acc: {best_acc*100:.2f}% | "
            f"Target: {target_acc*100:.0f}% {result_status}", 
            fontsize=14, fontweight='bold'
        )
        plt.tight_layout()
        plt.savefig(CONFIG["plot_path"], dpi=160, bbox_inches="tight")
        print(f"\nğŸ“ å›¾è¡¨å·²ä¿å­˜: {CONFIG['plot_path']}")


if __name__ == "__main__":
    # æ˜¾ç¤ºä½¿ç”¨è¯´æ˜
    print("\n" + "="*60)
    print("ç¥ç»ç½‘ç»œå®éªŒï¼šMLP vs CNN åœ¨ MNIST ä¸Šçš„å¯¹æ¯”")
    print("="*60)
    print("å­¦ä¹ ç›®æ ‡:")
    print("1. ç½‘ç»œç»“æ„å‚æ•°å¯¹æ¨¡å‹ç²¾åº¦çš„å½±å“")
    print("2. é€šè¿‡æ‰‹åŠ¨è°ƒå‚æå‡æ¨¡å‹æ€§èƒ½çš„æ–¹æ³•") 
    print("3. CNN ç›¸æ¯” MLP åœ¨å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­çš„ä¼˜åŠ¿")
    print("="*60)
    print("\nè¿è¡Œæ–¹æ³•:")
    print("1. è¿è¡ŒMLPå®éªŒï¼ˆç›®æ ‡98%ï¼‰:")
    print("   åœ¨CONFIGä¸­å°† 'model' è®¾ç½®ä¸º 'mlp'")
    print("   è¿è¡Œ: python bp_vs_cnn_mnist.py")
    print()
    print("2. è¿è¡ŒCNNå®éªŒï¼ˆç›®æ ‡99%ï¼‰:")
    print("   åœ¨CONFIGä¸­å°† 'model' è®¾ç½®ä¸º 'cnn'")
    print("   è¿è¡Œ: python bp_vs_cnn_mnist.py")
    print("="*60 + "\n")
    
    # æ£€æŸ¥æ˜¯å¦å®‰è£…äº†å¿…è¦çš„åº“
    try:
        import torch
        import torchvision
        print("âœ… PyTorchå’Œtorchvisionå·²å®‰è£…")
    except ImportError:
        print("âŒ è¯·å…ˆå®‰è£…PyTorch:")
        print("   pip install torch torchvision torchaudio")
        print("   æˆ–ä½¿ç”¨: pip install torch torchvision torchaudio -i https://pypi.tuna.tsinghua.edu.cn/simple")
        exit(1)
    
    main()