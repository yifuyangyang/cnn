1.网络结构参数对模型精度的影响
2.通过手动调参提升模型性能的方法
3.CNN 相比 MLP 在图像分类任务中的优势

4.在代码顶部 CONFIG 中选择模型：
"model": "mlp"   # 或 "cnn"


5.BP 神经网络（MLP）：通过调参，使 测试集准确率达到 98% 及以上

6.卷积神经网络（CNN）：通过调参，使 测试集准确率达到 99% 及以上

7.训练参数调整（可选）
可在 CONFIG 中修改：
学习率 lr
训练轮数 epochs
批大小 batch_size

8.作业提交内容（必须上传）

请提交以下内容：

（1）实验代码
修改后的 cnn.py

（2）结果截图或图片
MLP 达到 ≥98% 准确率的运行结果截图
CNN 达到 ≥99% 准确率的运行结果截图

（3）对应的训练曲线图（results.png，可多张）

（4）实验说明
简要说明所采用的网络结构和最终准确率
可写在 README.md 末尾或单独提交文档

9.最终提交PR时，请将标题修改为：学号+姓名，1979801234张三

MLP网络结构:
输入层：28×28×1 → Flatten(784)
第一层：全连接 784 → 512，ReLU，Dropout(0.3)
第二层：全连接 512 → 256，ReLU，Dropout(0.3)  
第三层：全连接 256 → 128，ReLU
输出层：全连接 128 → 10（softmax）
最终准确率：98.26%
MLP通过Flatten丢失了空间结构信息
需要更大容量（3层隐藏层）来补偿空间信息损失
Dropout有效控制了过拟合，提升了泛化能力
CNN网络结构：
输入层：28×28×1
第一层：Conv2d(1→32, 3×3) → BatchNorm → ReLU → MaxPool(2×2) → 14×14×32
第二层：Conv2d(32→64, 3×3) → BatchNorm → ReLU → MaxPool(2×2) → 7×7×64
第三层：Conv2d(64→128, 3×3) → BatchNorm → ReLU → MaxPool(2×2) → 3×3×128
Flatten: 3×3×128 → 1152
全连接：1152 → 256，ReLU，Dropout(0.5)
输出层：256 → 10（softmax）
最终准确率：99.39%
CNN保持了空间结构，通过卷积提取局部特征
BatchNorm加速收敛并提高稳定性
参数效率更高
3×3卷积核和池化层有效降低计算量